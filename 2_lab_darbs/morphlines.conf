# Specify server locations in a SOLR_LOCATOR variable; used later in variable substitutions:
SOLR_LOCATOR : {
  # Name of solr collection
  collection : collection1
  
  # ZooKeeper ensemble
  zkHost : "$ZK_HOST"
  
  # Relative or absolute path to a directory containing conf/solrconfig.xml and conf/schema.xml
  # If this path is uncommented it takes precedence over the configuration stored in ZooKeeper.  
  # solrHomeDir : "example/solr/collection1"
  
  # The maximum number of documents to send to Solr per network batch (throughput knob)
  # batchSize : 100
}

# Specify an array of one or more morphlines, each of which defines an ETL 
# transformation chain. A morphline consists of one or more (potentially 
# nested) commands. A morphline is a way to consume records (e.g. Flume events, 
# HDFS files or blocks), turn them into a stream of records, and pipe the stream 
# of records through a set of easily configurable transformations on it's way to 
# Solr (or a MapReduceIndexerTool RecordWriter that feeds via a Reducer into Solr).
morphlines : [
  {
    # Name used to identify a morphline. E.g. used if there are multiple morphlines in a 
    # morphline config file
    id : morphline1 
    
    # Import all morphline commands in these java packages and their subpackages.
    # Other commands that may be present on the classpath are not visible to this morphline.
    importCommands : ["org.kitesdk.**", "org.apache.solr.**"]
    
    commands : [                    
      { 
        # Parse Avro container file and emit a record for each avro object
        readAvroContainer {
          # Optionally, require the input record to match one of these MIME types:
          # supportedMimeTypes : [avro/binary]
          
          # Optionally, use a custom Avro schema in JSON format inline:
          # schemaString : """<json can go here>"""
          
          # Optionally, use a custom Avro schema file in JSON format:
          # schemaFile : /path/to/syslog.avsc
        }
      } 
      
      { 
        extractAvroPaths {
          flatten : false
          paths : { 
            id : /id            
            text : /text      
            user_friends_count : /user_friends_count
            user_location : /user_location
            user_description : /user_description
            user_statuses_count : /user_statuses_count
            user_followers_count : /user_followers_count
            user_name : /user_name
            user_screen_name : /user_screen_name
            created_at : /created_at
            retweet_count : /retweet_count
            retweeted : /retweeted
            in_reply_to_user_id : /in_reply_to_user_id
            source : /source
            in_reply_to_status_id : /in_reply_to_status_id
            media_url_https : /media_url_https
            expanded_url : /expanded_url
          }
        }
      }
      
      # Consume the output record of the previous command and pipe another record downstream.
      #
      # convert timestamp field to native Solr timestamp format
      # e.g. 2012-09-06T07:14:34Z to 2012-09-06T07:14:34.000Z
      {
        convertTimestamp {
          field : created_at
          inputFormats : ["yyyy-MM-dd'T'HH:mm:ss'Z'", "yyyy-MM-dd"]
          inputTimezone : America/Los_Angeles
#          outputFormat : "yyyy-MM-dd'T'HH:mm:ss.SSSZ"                                 
          outputTimezone : UTC
        }
      }
      
      {
        sanitizeUnknownSolrFields {
          # Location from which to fetch Solr schema
          solrLocator : ${SOLR_LOCATOR}
          
          # renameToPrefix : "ignored_"
        }
      }  
            
      # log the record at DEBUG level to SLF4J
      { logDebug { format : "output record: {}", args : ["@{}"] } }    
      
      # load the record into a SolrServer or MapReduce SolrOutputFormat.
      { 
        loadSolr {
          solrLocator : ${SOLR_LOCATOR}
        }
      }
    ]
  }
]
